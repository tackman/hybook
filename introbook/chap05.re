= 深層学習への利用

Python は深層学習ライブラリが最も充実している言語です。
これらの Python の資産を利用して、Hy で深層学習が可能です。

本章では、代表的な深層学習フレームワークのひとつ PyTorch を例に取り、
Hy での深層学習ライブラリの利用方法を挙げていきます。
例として用いたフレームワークは PyTorch ですが、同様の方法で TensorFlow / Keras や
Chainer など Python 向け深層学習ライブラリは利用できるでしょう。

== フレームワークの利用法

基本的な使い方は Python と変わりません。

//emlist{
(import torch)

(setv x (torch randn 5 3 100 120))


x.shape
;; Size(5, 3, 100, 120)
//}

Hy は Python と完全な互換性があるので、Python で書かれた各種コードは機械的に Hy に置き換え可能です。

Python コードを置き換えただけの Hy コードでは、特に Hy を使っているメリットを享受できるわけではありません。
しかし、最悪でも Python と同等の書き方ができるという事実はある種の保証になります。
「Hy でなく Python でなら実装できていた」ということはありえないので、安心して Hy を採用可能です。

Hy で深層学習に取り組む時は、まずサンプルの Python を Hy に書き写してみて、
そこから改善していくというのは一つの方法になるでしょう。

== Hy のパフォーマンス

機械学習は多量のコンピューティング資源を消費します。Hy は元から早いとは言えない Python ランタイムの上で、
さらに Hy の処理系を走らせているのでお世辞にも高速な部類ではないでしょう。
そのような処理系で機械学習をして大丈夫でしょうか？

この点に関しては、深層学習フレームワークを適切に利用する限りは、Python に比べて大きな問題は起きません。
そして世の深層学習は多くの場合は Python で回せています。

深層学習の速度で、でボトルネックになりそうな部分を挙げていくと、以下のようになるでしょう。

 1. GPU 上での処理速度
 2. データの IO の速度（GPU へのデータ転送も IO の一種とみなせます）
 3. スクリプト上での処理

このうち、Hy が低速なことで影響を受けるのは 3 番目です。ところで、このあたりの事情は Python でも変わりません。
そのため、深層学習フレームワークでは 3 番目の処理で重いものは無くて済むように設計されています。
Hy での深層学習フレームワークでも同様の利用をするため、Python と比べて多少低速であることは問題にならないでしょう。

パフォーマンスについては、第6章でベンチマークも混じえた考察を行っています。

== Jupyter の利用について

機械学習では Jupyter の利用も一般的で、Python では Jupyter 上で全てを片付けてしまうスタイルも一般的です。
Hy にも Jupyter のカーネルがありますが、残念ながら Python 並のサポートは受けられないのが現状です。

筆者が試した限り、Hy カーネルは

 * 動作が不安定。Jupyter 自体の再起動が必要になる場合が少なくない
 * GNUPlot などの描画機能に対応していない

など、Jupyter を敢えて使うメリットが少ない状態です。
Hy カーネルの開発に比べて Jupyter 自体の進化が早いので、誰かが気合を入れて Hy カーネル開発にコミットしない限り
この状況は改善しないと見込まれます。

ではどうするかという話ですが、エディタ上での REPL 実行をする REPL 駆動開発で
Jupyter のセル単位の実行に近いスタイルをすることはできます。
（REPL 駆動の具体的な方法に関しては、3 章「REPL 駆動」の節を参照）
元よりコーディング自体はブラウザ上でやるより Emacs 等のエディタ上でやる方が快適なので、
コーディング＆コード実行のサイクルは慣れればブラウザ上でやるより楽にできます。


== 試論：マクロの利用による静的検査

本節では、深層学習フレームワークにHyのマクロのラッパーを作成することで、
コンパイル時に可能な検査をしてしまう手法を提示します。

本節の内容はまだ検証前の段階で、本格的な実用を行える段階には至っていません。
そのため「試論」とした上で、潜在的なHyの価値を示すために敢えて掲載しています。

著者は現在実装中の深層学習タスクで本節の内容を実戦利用の予定があるため、
本書のバージョンアップ時には「試論」が取れた内容でお届けをする予定です。

=== Conv 算

深層学習フレームワークの利用者なら、入力した配列の次元が期待される値と違ってエラーを起こしたことがあります（断言）。

例えば、PyTorch の 2 次元畳み込みは次のような API を持っています。

//emlist{
（PyTorchドキュメントから引用すること）
class Conv2d(in_channels, out_channels_, stride, padding, ...)
//}

第 1 引数に注目してください。ここでは入力チャンネル数をユーザーが明示的に指定する必要があります。
もしここで指定したチャンネル数と異なる入力をした場合、実行時エラーが発生します。
このような形で入力次元に制約のある API は、深層学習ライブラリでは一般的です。

この手の次元不一致はたいてい実行時になって判明します。
アプリケーション制作やシステムプログラミングと違い、条件分岐を多用することはあまりない機械学習のコードでは
埋まったまま見つかりづらいバグになることは少ないでしょう。
しかし、深層学習の場合は一度の実行に長い時間がかかります。コードパスの後ろの方でこの手のバグがあった場合、
実行開始から何時間も経ってから実行時エラーからのやり直しになることもあります。

つまり、静的解析の需要があります。そして、この問題は Python では解決困難です。
ここで Hy のマクロを利用することで、実行前のエラー検出をする手法が考えられます。


前提を整理すると、

 * 実行前に配列次元が固定されている。実行時に配列次元を可変にする必要性は、深層学習ではあまりありません
 * コンパイル時に配列次元数の検査をしたい

以上の要件を、マクロの利用で解決していきます。

=== 準備

深層学習に適用する前に、よりシンプルな状況でマクロを構成してみます。
「ある関数が実行される前に、引数が前提を満たすか assert をかける」を目標にします。

次のような関数があったとします。

//emlist{
(defn f1 [n]
  (print "f1" n)
  (* n 2))

(defn f2 [n]
  (print "f2" n)
  (* n 2))


;;; 次のような呼び出しがをする予定

(setv x (f1 10))
(setv x (f2 x))
//}

それぞれ引数を取って、その値を表示します。与えられた引数によって決まる返り値を返す、単純な関数です。
print フォームは任意の副作用と考えれば、このような形式の関数は実践でも一般的です。
各関数内の副作用ーこの場合は print ーが実行される前に、引数が特定の値であるかの検査をする仕組みを作りましょう。

基本的なアイディアは、以下のようになります。

 * f1, f2 関数それぞれのラッパーマクロ f1*, f2* を作り、利用側はそれらを呼び出す
 * マクロ展開時に assertion をかける
 * マクロの展開後は、通常の関数呼び出しと同様の展開形にしたい

これを踏まえて実装すると、下記のようなマクロができます。

//emlist{
(defmacro f1* [n]
  (assert (= 100 n))
  (, `(f1 ~n) n))

(defmacro f2* [t]
  (assert (odd? (second t)))
  (, `(f2 (first ~t)) (* (second t) 2)))

;;; 利用法

(f2* (f1* 100))

;;; Threading macroも使える

(-> (f1* 100)
    (f2*))
//}

マクロ f1*は、引数 n に対して以下の 2 段階の処理をします。

 1. n の値への assertion
 2. n を使った関数 f1 の呼び出しと、n を含んだタプルを展開形にする

ここで、1 段目の処理はマクロ展開時に評価がされます。マクロの展開後にはマクロの「戻り値」だけが残るため、
マクロ f1*内での assert は展開後には残りません。
結果として、(f1* 100) のような呼び出しは、マクロの展開後は (, `(f1 100) 100) のようになります。

このマクロ f1\*は、引数 n が 100 であることを想定しています。n に 100 以外の値を入れて呼び出した場合に、何が起こるでしょうか？

仮に (f1* 2) のような呼び出しをしたとします。マクロ展開時に、マクロ f1*内のフォームが順番に評価され、
(assert (= 100 2))が評価されることになります。
この時点で AssertError が発生し、コンパイラはエラー発生として中断します。
これはマクロ展開時、つまりプログラム本体の実行前であることに注目です！実際、次のようなプログラム

//emlist{
(print "started!")

(f1* 2)
//}

を実行すると、"started!" が表示される _前に_ AssertionError が発生して、コンパイラが停止します。

マクロ f2* は、マクロ f1* の展開後のタプル t を取り、以下の 2 段階の処理をします。

 1. 引数タプル t の 2 番目の値で assertion
 2. 引数タプル t の 1 番目の値での関数 f2 の呼び出しと、2 番目の値を使ってタプルの 2 番目の戻り値を構成

assertion と展開形に関しては、マクロ f1\*と同様です。これらを合わせて、

//emlist{
(f2* (f1* 100))
//}

とすると、概念的には下記のように評価と展開されます。

//emlist{
1. (f1\* 100) の展開が開始
   1. (assert (= 100 100)) が評価され、検査をパス
   2. 展開後の (, `(f1 100) 100)が残る
2. (f2\* (, `(f1 100) 100)) の展開が開始
   1. (assert (odd? (second (, `(f1 100) 100)))) が評価され、検査をパス
   2. 展開形が下記のように評価される


(, `(f2 (first (, `(f1 100) 100))) (* (second (, `(f1 100) 100)) 2))

; 展開型タプルの2番目はこの時点で評価されるので、結果的な展開型は

(, `(f2 (first (, `(f1 100) 100))) (* 100 2))
; ↓ (* 100 2)のフォームが評価され、最終形
(, `(f2 (first (, `(f1 100) 100))) 200)
//}

f1* と f2* の返り値・引数は、関数の評価をクオートしたものと引数を評価した値のペアにしています。
これにより、「f1* に与えられた引数の性質を引き継いで f2* でも検査を行う」ことが可能になります。
これは f1* が単純に関数 f1 を評価するフォームだけを返しているとできないことです。
関数の戻り値が自明なこの例だと意味を感じにくいですが、次節以降の深層学習 API を利用する時には活用できる性質です。

=== ニューラルネットモジュールへの適用

前節までの内容で、ニューラルネットの静的検査をするためのテクニックは揃っています。
違いは適用する関数が複雑になっていることだけで、マクロの基本的な構成は同じです。

//emlist{
;;; nn.Conv2dのラッパー
(defmacro conv2d* [prev Cout kernel-size stride padding dilation]
  (setv (, N Cin Hin Win nets) (eval prev))

  (setv Hout (int (/ (+ Hin (* 2 padding) 
    (- (* dilation (- kernel-size 1))) -1) stride)))
  (setv Wout (int (/ (+ Win (* 2 padding) 
    (- (* dilation (- kernel-size 1))) -1) stride)))


  (,  N Cout Hout Wout 
  `[~nets ``(nn.Conv2d ~~~Cin ~~~Cout ~~~kernel-size 
    ~~~stride ~~~padding ~~~dilation)]))

;;; nn.BatchNorm2dのラッパー
(defmacro bn2d* [prev]
  (setv (, N Cin Hin Win nets) (eval prev))
;  (print nets)

  (setv num-features (* N Cin Hin Win))

  (, N Cin Hin Win `[~nets ``(nn.BatchNorm2d ~~~num-features)]))


(setv net (-> (conv2d* (,  5 3 100 200 `[]) 30 1 1 0 1)
              (bn2d* )
              (conv2d* 35 1 1 0 1))

;; net にnn.Conv2dとnn.BatchNorm2dのリストが入る
;; リストがネストした汚い形になるので、利用時に適宜falttenする必要はあります
//}

上記は畳み込み（Conv2d）とBatch Normalization(BatchNorm2d)を挟む、
畳み込みニューラルネットの一部でよく見るパターンです。
マクロを利用して、コンパイル時に入力次元数を計算して引数にしています。

これでニューラルネットの静的検査をしながらモデルが組める！大勝利！！
…と言いたいところですが、上記のプログラムはネットワークの深さが一定以上になると

//emlist{
hy.errors.HyCompileError: Internal Compiler Bug 😱
//}

なメッセージが出て死んでしまいます。
本書初版執筆時点では時間切れで解決できませんでしたが、近日中に解決策を導いて
改訂時には「試論：」を取れるようにする予定です。
